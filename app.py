import streamlit as st
import pandas as pd
import glob
import os
import html
import re
from datetime import datetime


# ==========================
# Tempostar CSV èª­ã¿è¾¼ã¿
# ==========================
@st.cache_data
def load_tempostar_data(file_paths):
    dfs = []
    for path in file_paths:
        df = pd.read_csv(path, encoding="cp932")
        df["å…ƒãƒ•ã‚¡ã‚¤ãƒ«"] = os.path.basename(path)
        dfs.append(df)

    all_df = pd.concat(dfs, ignore_index=True)

    # æ•°å€¤åˆ—
    for col in ["å¢—æ¸›å€¤", "å¤‰å‹•å¾Œ"]:
        if col in all_df.columns:
            all_df[col] = pd.to_numeric(all_df[col], errors="coerce").fillna(0).astype(int)

    return all_df


# ==========================
# å•†å“ç”»åƒãƒã‚¹ã‚¿èª­ã¿è¾¼ã¿
# ==========================
@st.cache_data
def load_image_master():
    folder = "å•†å“ç”»åƒURLãƒã‚¹ã‚¿"
    paths = glob.glob(os.path.join(folder, "*.csv"))

    if not paths:
        return {}

    dfs = []
    for p in paths:
        df = pd.read_csv(p, encoding="cp932")
        if (
            "å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰" in df.columns
            and "å•†å“ç”»åƒãƒ‘ã‚¹1" in df.columns
        ):
            dfs.append(df[["å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰", "å•†å“ç”»åƒãƒ‘ã‚¹1"]])

    if not dfs:
        return {}

    merged = pd.concat(dfs, ignore_index=True)
    merged["å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰"] = merged["å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰"].astype(str).str.strip()
    merged["å•†å“ç”»åƒãƒ‘ã‚¹1"] = merged["å•†å“ç”»åƒãƒ‘ã‚¹1"].astype(str).str.strip()

    return dict(zip(merged["å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰"], merged["å•†å“ç”»åƒãƒ‘ã‚¹1"]))


# ==========================
# HTML ãƒ†ãƒ¼ãƒ–ãƒ«ç”Ÿæˆ
# ==========================
def make_html_table(df):
    thead = "<thead><tr>" + "".join(
        f"<th>{html.escape(str(c))}</th>" for c in df.columns
    ) + "</tr></thead>"

    body_rows = []
    for _, row in df.iterrows():
        tds = []
        for col in df.columns:
            v = row[col]
            if col == "ç”»åƒ":
                tds.append(f"<td>{v}</td>")
            else:
                tds.append(f"<td>{html.escape(str(v))}</td>")
        body_rows.append("<tr>" + "".join(tds) + "</tr>")

    tbody = "<tbody>" + "".join(body_rows) + "</tbody>"

    return f"""
    <table border="1" cellspacing="0" cellpadding="4">
        {thead}{tbody}
    </table>
    """


# ==========================
# Main
# ==========================
def main():
    st.set_page_config(page_title="Tempostar å£²ä¸Šé›†è¨ˆï¼ˆç”»åƒä»˜ãï¼‰", layout="wide")

    st.title("Tempostar åœ¨åº«å¤‰å‹•ãƒ‡ãƒ¼ã‚¿ - SKUåˆ¥å£²ä¸Šé›†è¨ˆï¼ˆå•†å“ç”»åƒä»˜ãï¼‰")

    # ---------- CSVä¸€è¦§ ----------
    raw_paths = sorted(glob.glob("tempostar_stock_*.csv"))
    if not raw_paths:
        st.error("tempostar_stock_*.csv ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜æŠ½å‡º
    file_infos = []
    pat = re.compile(r"tempostar_stock_(\d{8})")

    for path in raw_paths:
        name = os.path.basename(path)
        m = pat.search(name)
        if m:
            d = datetime.strptime(m.group(1), "%Y%m%d").date()
            file_infos.append({"date": d, "path": path, "name": name})

    if not file_infos:
        st.error("tempostar_stock_YYYYMMDD.csv ã®å½¢å¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    all_dates = sorted({fi["date"] for fi in file_infos})
    min_date, max_date = min(all_dates), max(all_dates)

    # ---------- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ----------
    with st.sidebar:
        st.header("é›†è¨ˆæ¡ä»¶")

        st.write(f"ğŸ“… ãƒ‡ãƒ¼ã‚¿æœŸé–“ï¼š **{min_date} ã€œ {max_date}**")

        # â˜… ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã®ä»£ã‚ã‚Šã«ã€Œæ—¥ä»˜ãƒ—ãƒ«ãƒ€ã‚¦ãƒ³ï¼ˆæ•°å­—ã®ã¿ï¼‰ã€ã‚’ä½¿ã† â˜…
        start_date = st.selectbox(
            "é›†è¨ˆé–‹å§‹æ—¥",
            options=all_dates,
            index=len(all_dates) - 1,
            format_func=lambda d: d.strftime("%Y/%m/%d"),
        )

        end_date = st.selectbox(
            "é›†è¨ˆçµ‚äº†æ—¥",
            options=all_dates,
            index=len(all_dates) - 1,
            format_func=lambda d: d.strftime("%Y/%m/%d"),
        )

        if start_date > end_date:
            st.warning("é–‹å§‹æ—¥ãŒçµ‚äº†æ—¥ã‚ˆã‚Šå¾Œã«ãªã£ã¦ã„ã¾ã™ã€‚è‡ªå‹•ã§ä¸¦ã³æ›¿ãˆã¾ã™ã€‚")
            start_date, end_date = end_date, start_date

        # æœŸé–“å†… CSV
        target = [fi for fi in file_infos if start_date <= fi["date"] <= end_date]
        if not target:
            st.error("é¸æŠç¯„å›²ã®CSVãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return

        paths = [fi["path"] for fi in target]

        st.caption("å¯¾è±¡CSVï¼š")
        for fi in target:
            st.caption(f"ãƒ»{fi['date']} : {fi['name']}")

        keyword = st.text_input("æ¤œç´¢ï¼ˆå•†å“ã‚³ãƒ¼ãƒ‰ / å•†å“åŸºæœ¬ã‚³ãƒ¼ãƒ‰ / å•†å“åï¼‰")
        min_total_sales = st.number_input(
            "å£²ä¸Šå€‹æ•°ã®ä¸‹é™ï¼ˆãƒ—ãƒ©ã‚¹å€¤ï¼‰", min_value=0, value=0
        )

    # ---------- CSVèª­è¾¼ ----------
    df = load_tempostar_data(paths)

    if keyword:
        cond = False
        for col in ["å•†å“ã‚³ãƒ¼ãƒ‰", "å•†å“åŸºæœ¬ã‚³ãƒ¼ãƒ‰", "å•†å“å"]:
            if col in df.columns:
                cond |= df[col].astype(str).str.contains(keyword, case=False)
        df = df[cond]

    # å¿…é ˆåˆ—
    required = {"å•†å“ã‚³ãƒ¼ãƒ‰", "å•†å“åŸºæœ¬ã‚³ãƒ¼ãƒ‰", "å¢—æ¸›å€¤"}
    if not required.issubset(df.columns):
        st.error("Tempostar CSV ã«å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
        return

    # ---------- å£²ä¸Šï¼ˆå—æ³¨å–è¾¼ã®ã¿ï¼‰ ----------
    if "æ›´æ–°ç†ç”±" in df.columns:
        df_sales = df[df["æ›´æ–°ç†ç”±"] == "å—æ³¨å–è¾¼"]
    else:
        df_sales = df.copy()

    agg_sales = {
        "å•†å“åŸºæœ¬ã‚³ãƒ¼ãƒ‰": "last",
        "å•†å“å": "last",
        "å±æ€§1å": "last",
        "å±æ€§2å": "last",
        "å¢—æ¸›å€¤": "sum",
    }

    sales_grouped = (
        df_sales.groupby("å•†å“ã‚³ãƒ¼ãƒ‰", dropna=False)
        .agg(agg_sales)
        .reset_index()
        .rename(columns={"å¢—æ¸›å€¤": "å¢—æ¸›å€¤åˆè¨ˆ"})
    )

    sales_grouped["å£²ä¸Šå€‹æ•°åˆè¨ˆ"] = -sales_grouped["å¢—æ¸›å€¤åˆè¨ˆ"]
    sales_grouped = sales_grouped[sales_grouped["å£²ä¸Šå€‹æ•°åˆè¨ˆ"] > 0]

    # ---------- åœ¨åº« ----------
    if "å¤‰å‹•å¾Œ" in df.columns:
        stock_group = (
            df.groupby("å•†å“ã‚³ãƒ¼ãƒ‰")
            .agg({"å¤‰å‹•å¾Œ": "last"})
            .reset_index()
            .rename(columns={"å¤‰å‹•å¾Œ": "ç¾åœ¨åº«"})
        )
        sales_grouped = sales_grouped.merge(stock_group, on="å•†å“ã‚³ãƒ¼ãƒ‰", how="left")

    # ---------- ãƒ•ã‚£ãƒ«ã‚¿ ----------
    if min_total_sales > 0:
        sales_grouped = sales_grouped[sales_grouped["å£²ä¸Šå€‹æ•°åˆè¨ˆ"] >= min_total_sales]

    sales_grouped = sales_grouped.sort_values("å£²ä¸Šå€‹æ•°åˆè¨ˆ", ascending=False)

    # ---------- ç”»åƒ ----------
    img_master = load_image_master()
    base_url = "https://image.rakuten.co.jp/hype/cabinet"

    def to_img(row):
        code = str(row["å•†å“åŸºæœ¬ã‚³ãƒ¼ãƒ‰"]).strip()
        rel = img_master.get(code, "")
        if not rel:
            return ""
        return f'<img src="{base_url + rel}" width="120">'

    sales_grouped["ç”»åƒ"] = sales_grouped.apply(to_img, axis=1)
    cols = sales_grouped.columns.tolist()
    cols.insert(0, cols.pop(cols.index("ç”»åƒ")))
    sales_grouped = sales_grouped[cols]

    # ---------- è¡¨ç¤º ----------
    display = [
        "ç”»åƒ",
        "å•†å“ã‚³ãƒ¼ãƒ‰",
        "å•†å“åŸºæœ¬ã‚³ãƒ¼ãƒ‰",
        "å•†å“å",
        "å±æ€§1å",
        "å±æ€§2å",
        "å£²ä¸Šå€‹æ•°åˆè¨ˆ",
        "ç¾åœ¨åº«",
        "å¢—æ¸›å€¤åˆè¨ˆ",
    ]

    df_view = sales_grouped[display]

    st.write(
        f"ğŸ“¦ SKUæ•°ï¼š{len(df_view):,}ã€€ï½œã€€é›†è¨ˆæœŸé–“ï¼š{start_date.strftime('%Y/%m/%d')} ã€œ {end_date.strftime('%Y/%m/%d')}"
    )

    # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºï¼ˆHTMLï¼‰
    table_html = make_html_table(df_view)

    st.markdown(
        """
    <style>
    table { border-collapse: collapse; font-size: 14px; }
    th { background:#f2f2f2; }
    td, th { padding:6px 8px; border:1px solid #ccc; }
    tr:hover { background:#fafafa; }
    img { display:block; }
    </style>
    """,
        unsafe_allow_html=True,
    )

    st.markdown(table_html, unsafe_allow_html=True)


if __name__ == "__main__":
    main()
